[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Psychology",
    "section": "",
    "text": "Welcome!\nThis book is written in mind for someone working in Psychology and is venturing into R with little to no experience. Research data in Psychology is unique in that it is collected in formats that are human-readable but not exactly R-readable. Data wrangling conventions will often vary depending on research question and therefore by what type data is collected. In ‚ÄúR for Psychology‚Äù, we have compiled a series of real-world examples from different sub-disciplines and will walk through the process of wrangling, summary, analyses and visualisations\nThis book is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\n\n\n\nHow to contribute\nThis book is built using Quarto and hosted via GitHub Pages. It is a living resource, we welcome any contributions from the Psychology community that would improve the quality of this book. There are many ways to contribute:\n\nFixing grammar and typos\nClarification or expanding on existing content\nContribute real-world data in worked examples\nAuthoring an entire chapter\n\nTake a look at our Contributing Guide to see how you can help!\n\n\nAcknowledgements\nThis book was created on the unceded territory of the Bedegal people who are the Traditional custodians of the lands where the Kensington campus is located.\nThe first version of this book was funded by the UNSW Research Infrastructure Scheme.\nWe would like to thank the following people who have contributed to the book:\n@daxkellie"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. ‚ÄúLiterate Programming.‚Äù Comput. J. 27 (2): 97‚Äì111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "survey.html",
    "href": "survey.html",
    "title": "2¬† Making Inferences",
    "section": "",
    "text": "Take it away Lisa!\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. ‚ÄúLiterate Programming.‚Äù Comput.\nJ. 27 (2): 97‚Äì111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "contributing.html#quickstart-reference",
    "href": "contributing.html#quickstart-reference",
    "title": "3¬† Contributing",
    "section": "3.1 Quickstart reference",
    "text": "3.1 Quickstart reference\n\n3.1.1 Install Quarto\nThis book\n\n\n3.1.2 Quarto Workflow\n\n\n3.1.3 GitHub Workflow"
  },
  {
    "objectID": "contributing.html#book-structure",
    "href": "contributing.html#book-structure",
    "title": "8¬† Contributing",
    "section": "8.1 Book Structure",
    "text": "8.1 Book Structure"
  },
  {
    "objectID": "contributing.html#book-practices-and-conventions",
    "href": "contributing.html#book-practices-and-conventions",
    "title": "8¬† Contributing",
    "section": "8.2 Book Practices and Conventions",
    "text": "8.2 Book Practices and Conventions"
  },
  {
    "objectID": "contributing.html#book-edits",
    "href": "contributing.html#book-edits",
    "title": "3¬† Contributing",
    "section": "3.4 Book Edits",
    "text": "3.4 Book Edits\n\n3.4.1 Edits\n\n\n3.4.2 Additions"
  },
  {
    "objectID": "contributing.html#contribution-review",
    "href": "contributing.html#contribution-review",
    "title": "8¬† Contributing",
    "section": "8.4 Contribution Review",
    "text": "8.4 Contribution Review"
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "1¬† Introduction",
    "section": "1.1 What you will learn",
    "text": "1.1 What you will learn\nHow to:\n\nRead your hard-earned data into R\nWrangle and clean the data in a R-friendly format\nProduce summary statistics\nAnalyse your data\nVisualise your findings\n\nPre-analysis stages:\n\n\n\nAnalysis stages:"
  },
  {
    "objectID": "intro.html#how-book-is-organised",
    "href": "intro.html#how-book-is-organised",
    "title": "1¬† Introduction",
    "section": "1.2 How book is organised",
    "text": "1.2 How book is organised\nThis book is organised by data type (e.g.¬†Survey data, Questionnaire data).\nEach chapter will walk through the process will work with real-world Psychology data and will walk you through reading in data, to cleaning and eventually analysis and visualising the results.\n\n1.2.1 Conventions\nWe will refer to packages as {dplyr} and functions as mean(). Variables and objects (such as file names or data objects) as age and mtcars. Where it would aid understanding, we will sometimes refer to functions within a particular packages as dplyr::mutute()"
  },
  {
    "objectID": "intro.html#prerequisites-content-drawn-from-existing-resources-such-as-httpsr4ds.hadley.nzintroprerequisites",
    "href": "intro.html#prerequisites-content-drawn-from-existing-resources-such-as-httpsr4ds.hadley.nzintroprerequisites",
    "title": "1¬† Introduction",
    "section": "1.3 Prerequisites (Content drawn from existing resources such as https://r4ds.hadley.nz/intro#prerequisites)",
    "text": "1.3 Prerequisites (Content drawn from existing resources such as https://r4ds.hadley.nz/intro#prerequisites)\n\n1.3.1 R\nDownload Point to intro to R content (RUWithme, Environmental Computing, Software Carpentry)\n\n\n1.3.2 RStudio\nRStudio projects Point to resource about Rproj (SWC)\nRunning R code https://r4ds.hadley.nz/intro#running-r-code\n\n\n1.3.3 Version control with git\n\n\n1.3.4 R packages\nEvery code section will always begin with calls to R packages. There will be commented out code for you to install these.\nHere are few packages we consider essential to working with Psychology data.\n\ntidyverse\nggplot2\nhere\njanitor\n\nAt the end of each chapter, we will also include our call to sessionInfo() so you can see what version of which packages we are using.\n\n\n1.3.5 Virtual environments\nSpeaking on what package versions we are using as we write this book, we understand the R package space is constantly changing. This means sometimes code will break due to package updates and this is all part of the process! To combat this problem, we‚Äôve enlisted renv to create a reproducible environment for building this book.\n\n1.3.5.1 Download our virtual environment\nThe virtual environment used to build this book is stored in a lockfile. You can find this file in the GitHub repository where the source code of this book lives.\nThe lockfile is named renv.lock. You can download this file directly but clicking on the file name and clicking on the ‚ÄúDownload raw file‚Äù button.\n\nAlternatively, you can clone our repository into your computer. Learn more about cloning repositorsies and other GitHub workflows in Happy Git by Jenny Bryan.\nOnce you have this file downloaded, move it in a relevant project directory and then we can let {renv} work its magic.\n\n\n1.3.5.2 Install\nFirst things first, lets install renv if we don‚Äôt have it already.\n\ninstall.packages(\"renv\")\n\nlibrary(renv)\n\n\n\n1.3.5.3 Recreate virtual enviroment\nNow let‚Äôs tell renv where our downloaded renv.lock file is. Specific the path to the file in the function restore() and you are good to go!\n\nrestore(lockfile = \"path_to_renv.lock\")"
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1¬† Introduction",
    "section": "1.3 Prerequisites",
    "text": "1.3 Prerequisites\nContent drawn from existing resources such as https://r4ds.hadley.nz/intro#prerequisites\n\n1.3.1 R\nDownload Point to intro to R content (RUWithme, Environmental Computing, Software Carpentry)\n\n\n1.3.2 RStudio\nRStudio projects Point to resource about Rproj (SWC)\nRunning R code https://r4ds.hadley.nz/intro#running-r-code\n\n\n1.3.3 Version control with git\n\n1.3.3.1 What is git?\n\n\n1.3.3.2 Why do I need git?\n\n\n\n\n1.3.4 R packages\nEvery code section will always begin with calls to R packages. There will be code that is commented out (have # preceding the code) for you to install these if you don‚Äôt have them on your computer\n\n# install.packages(dplyr)\n\nlibrary(dplyr)\n\nThere are few R packages that will be on heavy rotation when it comes to working with Psychology data. #### {tidyverse}\n{tidyverse} is a collection of R packages that is essential to a data scientist‚Äôs toolkit. By installing {tidyverse} you are actually installing 8 other packages. The ones we will most often use include:\n\n{dplyr}\n{ggplot2}\n{tidyr}\n\nThe handy thing is, when you load the {tidyverse} library into R, it will load the core suite of packages for you instead of you loading each of them independently! Efficiency!! üöÄ\n\nlibrary(tidyverse)\n\nOther packages that will be helpful for your R workflows include:\n\n{here}\n\n{janitor}\n\nAt the end of each chapter, we will also include our call to sessionInfo() so you can see what version of packages we are using."
  },
  {
    "objectID": "intro.html#virtual-environments",
    "href": "intro.html#virtual-environments",
    "title": "1¬† Introduction",
    "section": "1.4 Virtual environments",
    "text": "1.4 Virtual environments\nSpeaking on what package versions as we write this book, we understand the R package space is constantly changing. This means sometimes code will break due to package updates and this is all part of the process! To combat this problem, we‚Äôve enlisted renv to create a reproducible environment for building this book.\n\n1.4.1 Download our virtual environment\nThe virtual environment used to build this book is stored in a lockfile. You can find this file in the GitHub repository where the source code of this book lives.\nThe lockfile is named renv.lock. You can download this file directly but clicking on the file name and clicking on the ‚ÄúDownload raw file‚Äù button.\n\nAlternatively, you can clone our repository into your computer. Learn more about cloning repositorsies and other GitHub workflows in Happy Git by Jenny Bryan.\nOnce you have this file downloaded, move it in a relevant project directory and then we can let {renv} work its magic.\n\n\n1.4.2 Install\nFirst things first, lets install renv if we don‚Äôt have it already.\n\ninstall.packages(\"renv\")\n\nlibrary(renv)\n\n\n\n1.4.3 Recreate virtual enviroment\nNow let‚Äôs tell renv where our downloaded renv.lock file is. Specific the path to the file in the function restore() and you are good to go!\n\nrestore(lockfile = \"path_to_renv.lock\")"
  },
  {
    "objectID": "contributing.html#book-contributions",
    "href": "contributing.html#book-contributions",
    "title": "8¬† Contributing",
    "section": "8.3 Book Contributions",
    "text": "8.3 Book Contributions\n\n8.3.1 Edits\n\n\n8.3.2 Additions"
  },
  {
    "objectID": "inferences.html",
    "href": "inferences.html",
    "title": "7¬† Infer",
    "section": "",
    "text": "8 Moving into analyses\nlw_addnarrativehere\nthis asks the lm package to use predictor to predict outcome\nlm(outcome ~ predictor )\nregression1 &lt;- lm(data = data_scalescomputed, variable6 ~ demographicscateg + variable4 + scale1_index)\n\nsummary(regression1)\nlw_addnarrativehere simulates hierarchical stepwise regression?\nregressionlm1 &lt;- lm(data = data_scalescomputed, variable6 ~ demographicscateg)\nregressionlm2 &lt;- lm(data = data_scalescomputed, variable6 ~ demographicscateg + variable4)\nregressionlm3 &lt;- lm(data = data_scalescomputed, variable6 ~ demographicscateg + variable4 + scale1_index)\n\nsummary(regressionlm1)\nsummary(regressionlm2)\nsummary(regressionlm3)\n\n\napa.reg.table(regressionlm1, regressionlm2, regressionlm3, filename = here(\"output_files\",\"RegressionTable3_APA.doc\"), table.number = 3)\n\nreport(regressionlm3)\nlw add narrative\n4 (between participants - condition1234) x2 (between participants - individual difference categorical variable - demographicscateg) design\noutcome: scale1_index\n## first tell R that condition and demographicscateg are categorical (factors)\n\ndata_scalescomputed$condition1234 &lt;- as.factor(data_scalescomputed$condition1234)\ndata_scalescomputed$demographicscateg &lt;- as.factor(data_scalescomputed$demographicscateg)\n\n## this creates a dataframe that contains the summary statistics (means, standard deviations, etc.)\n\nscale1_stats &lt;- ezStats(data = data_scalescomputed, \n                            dv = scale1_index,\n                            wid = participantid,\n                            between = c(\"condition1234\",\"demographicscateg\"))\n                            \nprint(scale1_stats)\n\n## this carries out an anova with condition1234 and demographicscateg as between-participants factors\n## the output includes statistical tests of the main effect of condition1234, the main effect of demographicscateg, and the interaction between the two\n\nscale1_anova &lt;- ezANOVA(data = data_scalescomputed, \n                            dv = scale1_index,\n                            wid = participantid,\n                            between = c(\"condition1234\",\"demographicscateg\"),\n                            return_aov = TRUE)\n\nprint(scale1_anova)\n\n## NOTE THAT THE FOLLOWING WOULD BE DONE TO FURTHER INVESTIGATE AN INTERACTION OR A DIFFERENCE IN A MULTI-CATEGORICAL VARIABLE. \n## to build the contrasts we want to do (e.g., baseline vs. midpoint for PhD), we need to ask emmeans to create output to see what the rows represent.\n\nscale1_emm &lt;- emmeans(scale1_anova$aov, ~ condition1234 * demographicscateg)\n\nscale1_emm\n\n## now we can create a set of named vectors that represent each of the 8 means, based on the position in the 8-item list from emmeans.\n\ncond1categ1 = c(1,0,0,0,0,0,0,0)\ncond2categ1 = c(0,1,0,0,0,0,0,0)\ncond3categ1 = c(0,0,1,0,0,0,0,0)\ncond4categ1 = c(0,0,0,1,0,0,0,0)\ncond1categ2 = c(0,0,0,0,1,0,0,0)\ncond2categ2 = c(0,0,0,0,0,1,0,0)\ncond3categ2 = c(0,0,0,0,0,0,1,0)\ncond4categ2 = c(0,0,0,0,0,0,0,1)\n\ncond1 = c(1,0,0,0,1,0,0,0)\ncond2 = c(0,1,0,0,0,1,0,0)\ncond3 = c(0,0,1,0,0,0,1,0)\ncond4 = c(0,0,0,1,0,0,0,1)\n\n\n## now we can ask for contrasts based on these vectors; we will explore the main effect of condition1234 by comparing each of the conditions to one another\n\nscale1_contrasts &lt;- contrast(scale1_emm, method = list(\"cond1 - cond2\" = cond1 - cond2,\n                                       \"cond1 - cond3\" = cond1 - cond3,\n                                       \"cond1 - cond4\" = cond1 - cond4,\n                                       \"cond2 - cond3\" = cond2 - cond3,\n                                       \"cond2 - cond4\" = cond2 - cond4,\n                                       \"cond3 - cond4\" = cond3 - cond4)) \n  \n\nscale1_contrasts\n\n## here's a version with confidence intervals instead of t and p values\n\nscale1_contrastsCI &lt;- scale1_contrasts %&gt;%\n                          confint()\n\nscale1_contrastsCI\n\n#you may have a specific planned contrast to run. Here let's imagine you want to compare condition 2 to all of the other 3 conditions in one comparison.\n\nscale1_plannedcontraststats &lt;- ezStats(data = data_scalescomputed, \n                            dv = scale1_index,\n                            wid = participantid,\n                            between = c(\"condition1234\"))\n\nscale1_plannedcontraststats\n\ncond2v134 = c(0,1,0,0,0,1,0,0)\ncond134v2 = c(1,0,1,1,1,0,1,1)\n\nscale1_plannedcontrasts &lt;- contrast(scale1_emm, method = list(\"condition 1 vs all the rest\" = cond2v134 - cond134v2))\n  \nscale1_plannedcontrasts\nLet‚Äôs make a figure!\ndata_scalescomputed %&gt;%\n  ggplot(aes(x = condition1234, y = scale1_index, fill = demographicscateg)) +\n  geom_boxplot() +\n  theme_light()\n\n\ndata_scalescomputed %&gt;%\n  ggplot(aes(x = condition1234, y = scale1_index)) +\n  geom_boxplot() +\n  theme_light()"
  },
  {
    "objectID": "import.html#reading-in-excel",
    "href": "import.html#reading-in-excel",
    "title": "2¬† Import",
    "section": "2.1 Reading in Excel",
    "text": "2.1 Reading in Excel"
  },
  {
    "objectID": "import.html#reading-in-.csv",
    "href": "import.html#reading-in-.csv",
    "title": "2¬† Import",
    "section": "2.2 Reading in .csv",
    "text": "2.2 Reading in .csv"
  },
  {
    "objectID": "import.html#reading-in-spss",
    "href": "import.html#reading-in-spss",
    "title": "3¬† Import",
    "section": "3.2 Reading in SPSS",
    "text": "3.2 Reading in SPSS"
  },
  {
    "objectID": "import.html#reading-in-qualtrics",
    "href": "import.html#reading-in-qualtrics",
    "title": "2¬† Import",
    "section": "2.4 Reading in Qualtrics",
    "text": "2.4 Reading in Qualtrics\n\n2.4.1 Reading in .sav"
  },
  {
    "objectID": "import.html#reading-in-.sav",
    "href": "import.html#reading-in-.sav",
    "title": "2¬† Import",
    "section": "2.5 Reading in .sav",
    "text": "2.5 Reading in .sav"
  },
  {
    "objectID": "import.html#reading-in-qualtrics-data",
    "href": "import.html#reading-in-qualtrics-data",
    "title": "3¬† Import",
    "section": "3.3 Reading in Qualtrics data",
    "text": "3.3 Reading in Qualtrics data\n\n3.3.1 Reading in .sav"
  },
  {
    "objectID": "import.html#reading-in-excel-spreadsheets",
    "href": "import.html#reading-in-excel-spreadsheets",
    "title": "3¬† Import",
    "section": "3.1 Reading in Excel spreadsheets",
    "text": "3.1 Reading in Excel spreadsheets\nThis is\n\n3.1.1 Reading in .csv"
  },
  {
    "objectID": "organise.html#r-projects",
    "href": "organise.html#r-projects",
    "title": "2¬† Organise",
    "section": "2.1 R projects",
    "text": "2.1 R projects\nR really cares about where things live on your computer, even if you don‚Äôt. Humans have gotten out of the habit of thinking very hard about where they put things on their machine; the search capabilities on the modern computer are quite good and you can generally find files quite easily by searching for them.\nWhen you are coding in R, however, you need to explicitly tell R where to find things. You can make this process much easier for yourself by always working within an RStudio Project.\nWhen you work within an RStudio project, you can reference everything in relation to the top level of that project folder. It doesn‚Äôt matter where that project folder lives on your machine (i.e.¬†Desktop, Documents, OneDrive, Dropbox) the code you write is always relative to your project folder. This means that you can share that whole project folder with someone else (your collaborators or supervisor), and your code won‚Äôt break.\n\nWhen you start a new analysis project, create a New RStudio Project via the File tab.\n\n\n\n\nWorking in an RStudio Project called my_new_analysis\n\n\nAlways open RStudio by double clicking on the .RProj file in the folder on your machine. There is an icon in the top right corner of RStudio that shows you which project you are working in and makes it easy to switch between projects."
  },
  {
    "objectID": "organise.html#consistency-folder-structure",
    "href": "organise.html#consistency-folder-structure",
    "title": "2¬† Organise",
    "section": "2.3 Consistency folder structure",
    "text": "2.3 Consistency folder structure\nMaking directories"
  },
  {
    "objectID": "organise.html#naming-things",
    "href": "organise.html#naming-things",
    "title": "2¬† Organise",
    "section": "2.4 naming things",
    "text": "2.4 naming things\nWhen naming things in your analysis folder, it is a good idea to think about your future self. In all likelihood, when you come back to this analysis in a few months time, you will have no recollection how it actually worked, but you can leave yourself some breadcrumbs by being a bit intentional about naming things.\nYour file names should be meaningful and orderly. The name of each file should tell the new user (or future you) what is in the file and where it goes in the process.\n\ncleaning-functions.R\n1_wrangle.Rmd\n2_visualise.Rmd\n3_analyse.Rmd\n\nIn this project, I can tell by glancing at the file names that I have a script (.R) that contains functions and three R Markdown files that contain each stage of the analysis.\nSticking with lower case is a good idea; avoid special characters and use - or _ to fill any gaps.\nFind more useful naming tips in the Tidyverse Style guide."
  },
  {
    "objectID": "organise.html#documenting-things",
    "href": "organise.html#documenting-things",
    "title": "2¬† Organise",
    "section": "2.5 documenting things",
    "text": "2.5 documenting things\n\n2.5.1 README\nIn addition to the breadcrumbs that our file names leave, it is also a good idea to leave explicit notes to your future self (or someone else) in a README.md file. This is a simple text file that contains instructions for how the user should engage with your project.\nCreate a new Markdown file and save it as README.md\nUse it to leave yourself instructions that look a bit like this.\n\n\n\nAn example README file\n\n\n\n\n2.5.2 R Markdown\nIn addition to leaving your future self explicit notes about how to engage with the project generally in a README document, it is also best practice to document your code in a way that makes it really clear what the code is doing and why. For this reason, we recommend using R Markdown documents (rather than R scripts) for your analysis.\nR Markdown is a handy file format that allows you to intersperse chunks of code with notes. This kind of document makes it easy to write explanations, interpretations, and thoughts for your future self as you code. This kind of documentation makes it much more likely to be able to make sense of what you have done and why, when you come back to your analysis in a few months time.\nR Markdown documents can also be ‚Äúknitted‚Äù into html, pdf, or word documents, allowing you to share your analysis (and associated thoughts) with collaborators, even if they don‚Äôt know R.\nTo get up to speed on R Markdown and how to use it, check out RLadiesSydney #RYouWithMe MarkyMark module"
  },
  {
    "objectID": "organise.html#section",
    "href": "organise.html#section",
    "title": "2¬† Organise",
    "section": "2.5 ",
    "text": "2.5"
  },
  {
    "objectID": "organise.html#sections-of-your-doc",
    "href": "organise.html#sections-of-your-doc",
    "title": "2¬† Organise",
    "section": "2.6 Sections of your doc",
    "text": "2.6 Sections of your doc"
  },
  {
    "objectID": "wrangle.html#clean-names",
    "href": "wrangle.html#clean-names",
    "title": "4¬† Wrangle",
    "section": "4.1 Clean names",
    "text": "4.1 Clean names"
  },
  {
    "objectID": "wrangle.html#dealing-with-labels",
    "href": "wrangle.html#dealing-with-labels",
    "title": "4¬† Wrangle",
    "section": "4.2 Dealing with labels",
    "text": "4.2 Dealing with labels"
  },
  {
    "objectID": "wrangle.html#exclusions",
    "href": "wrangle.html#exclusions",
    "title": "4¬† Wrangle",
    "section": "4.3 Exclusions",
    "text": "4.3 Exclusions\nmutate case_when\nfilter"
  },
  {
    "objectID": "wrangle.html#creating-scales-and-indexes",
    "href": "wrangle.html#creating-scales-and-indexes",
    "title": "4¬† Wrangle",
    "section": "4.4 Creating scales and indexes",
    "text": "4.4 Creating scales and indexes\ngroup_by summarise\nrow_wise mutate\n\n4.4.1 Checking reliabilty"
  },
  {
    "objectID": "organise.html#working-in-an-rstudio-projectusing-the-here-package.",
    "href": "organise.html#working-in-an-rstudio-projectusing-the-here-package.",
    "title": "2¬† Organise",
    "section": "2.2 using the here() package.",
    "text": "2.2 using the here() package.\nOpen your project by double clicking on the RProj file. There is an icon in the top right corner of RStudio IDE that shows you which project you are working in and makes it easy to switch between projects."
  },
  {
    "objectID": "organise.html#where-is-here",
    "href": "organise.html#where-is-here",
    "title": "2¬† Organise",
    "section": "2.2 where is here?",
    "text": "2.2 where is here?\nOnce you have set up a project to contain your analysis, you can avoid further file path drama by using the here() package. This package makes it super easy to refer to file paths and ensures that your code will work, even if someone else is trying to run it on a different computer.\nOnce you have installed the here() package, use it to tell R where you find your data like this. This code chunk loads there here() and readr() packages and then tells R to read in a csv file, located in the ‚Äúdata‚Äù folder, and the file is called file.csv\n\nmydata &lt;- read_csv(here(\"data\", \"file.csv\"))\n\nBy referring to the location of your data using the here() package, here is no need worry about working directories, and you can be sure that your code will work on any machine.\n\nTo read more about why projects and the here() package are useful, check out this blog by Jenny Bryan"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "10¬† Appendix",
    "section": "",
    "text": "11 how to install R and RStudio on your machine\nThe marvellous Danielle Navarro has LOTS of useful R learning resources on her YouTube channel. This playlist about how to install R and RStudio is particularly useful; no matter which operating system you are dealing with‚Ä¶ Dani has you covered.\nYou only need to install a package once on your machine. Once the package is installed, you will need to use library() to load the functions in it every time you want to use it, but the installation is a one time job. So you can either do it in the console, or using the packages tab.\n#pulls from the exclusions_summary tabyl created above\n\ncomments_only_count &lt;- exclusions_summary %&gt;% filter(exclude_coded == \"comments only\") %&gt;% pull(n)\ncomments_only_count2 &lt;- exclusions_summary$n[which(exclusions_summary$exclude_coded==\"comments_only\")]\n\ntime_only_count &lt;- exclusions_summary %&gt;% filter(exclude_coded == \"time only\") %&gt;% pull(n)\nvariance_only_count &lt;- exclusions_summary %&gt;% filter(exclude_coded == \"variance only\") %&gt;% pull(n)\ntotal &lt;- exclusions_summary %&gt;% filter(exclude_coded == \"Total\") %&gt;% pull(n)\nkept &lt;- exclusions_summary %&gt;% filter(exclude_coded == \"kept\") %&gt;% pull(n)\nUse of inline code is really helpful in avoiding transcription errors and saving time when writing up! Here, we use code to pull in some descriptive statistics from the exclusion reason table we made above:\nOption-Command-I = inserts a new code chunk Command-Enter = runs the chunk of code that your cursor is in"
  },
  {
    "objectID": "appendix.html#option-1-in-the-console",
    "href": "appendix.html#option-1-in-the-console",
    "title": "10¬† appendix",
    "section": "11.1 option 1: in the console",
    "text": "11.1 option 1: in the console\ninstall.packages(\"packagename\")"
  },
  {
    "objectID": "appendix.html#option-2-in-the-packages-tab",
    "href": "appendix.html#option-2-in-the-packages-tab",
    "title": "10¬† appendix",
    "section": "11.2 option 2: in the packages tab",
    "text": "11.2 option 2: in the packages tab\nAlternatively, search for the package you would like to install in the packages tab.\n\n\n\nYou can search for packages and install them from CRAN via the packages tab\n\n\n\nRemember once you have installed a package, you will need to use the library() function to load it before it will work."
  },
  {
    "objectID": "appendix.html#option-1",
    "href": "appendix.html#option-1",
    "title": "10¬† Appendix",
    "section": "12.1 option 1",
    "text": "12.1 option 1\nInstall a package by typing the following command with the name of the package you would like to install in the console.\ninstall.packages(\"packagename\")"
  },
  {
    "objectID": "appendix.html#option-2",
    "href": "appendix.html#option-2",
    "title": "10¬† Appendix",
    "section": "12.2 option 2",
    "text": "12.2 option 2\nAlternatively, search for the package you would like to install in the packages tab.\n\n\n\nYou can search for packages and install them from CRAN via the packages tab\n\n\n\nRemember once you have installed a package, you will need to use the library() function to load it before it will work."
  },
  {
    "objectID": "organise.html#consist-folder-structure",
    "href": "organise.html#consist-folder-structure",
    "title": "2¬† Organise",
    "section": "2.3 Consist folder structure",
    "text": "2.3 Consist folder structure\nOnce you have your project set up, you might like to think about imposing some structure on it. It is mostly personal preference, but many analysis projects include the following folders.\n\ndata\n\nraw-data\nclean-data\n\noutput\n\nfigures\ntables\n\nmanuscript\n\nYou always want to keep your raw untouched data separate from any data that you have written after your data cleaning process separate, so a raw-data subfolder can be useful. You can write the figures and tables that you create to an output folder and put any writing that you are doing in the manuscripts folder.\nUsually the scripts (or Rmarkdown) documents that you write your code in, live in the top level of your project file.\nIn RStudio, your project structure might look something like this.\n\n\n\nA project structure template"
  },
  {
    "objectID": "organise.html#a-project-structure-templatenaming-things",
    "href": "organise.html#a-project-structure-templatenaming-things",
    "title": "2¬† Organise",
    "section": "2.4 Naming things",
    "text": "2.4 Naming things\nFiles, folders"
  },
  {
    "objectID": "organise.html#folder-structure",
    "href": "organise.html#folder-structure",
    "title": "2¬† Organise",
    "section": "2.3 folder structure",
    "text": "2.3 folder structure\nOnce you have your project set up, you might like to think about imposing some structure on it. It is mostly personal preference, but many analysis projects include the following folders.\n\ndata\n\nraw-data\nclean-data\n\noutput\n\nfigures\ntables\n\nmanuscript\n\nYou always want to keep your raw data untouched and separate from any data that you write back to your machine after your data cleaning process, so a raw-data subfolder can be useful.\nIn addition, you might want to organise your figures and tables into an output folder and put any writing that you are doing in the manuscripts folder.\n\nIf you want to write a manuscript (or maybe your whole thesis!) using RMarkdown, check out the papaja (Preparing APA Journal Articles) package\n\nUsually the scripts (or R Markdown) documents that you write your code in, live in the top level of your project file. In RStudio, your project structure might look something like this.\n\n\n\nA project structure template"
  },
  {
    "objectID": "inferences.html#plots-and-descriptives",
    "href": "inferences.html#plots-and-descriptives",
    "title": "7¬† Infer",
    "section": "8.1 Plots and descriptives",
    "text": "8.1 Plots and descriptives\nNow, let‚Äôs test whether this scale varies by condition (condition 1 vs.¬†condition 2 in condition12).\nBefore jumping into t-tests, let‚Äôs first visualise the data (scale1_index) to get a sense of what it looks like.\nThe first command here tells R to treat condition12 as a categorical variable that groups the data (technically, a factor). This is helpful for plotting. The code uses the dataframe$variablename structure to do a command on just one variable.\nThe second command pipes the data into a command to create a boxplot (geom_boxplot) with the datapoints plotted as well (geom_jitter).The width command makes the plotting of the dots a bit narrower to help with interpretation.\n\ndata_scalescomputed$condition12 &lt;- as.factor(data_scalescomputed$condition12)\n\ndata_scalescomputed %&gt;%\n  ggplot(aes(x = condition12, y = scale1_index, fill = condition12)) +\n  geom_boxplot(alpha = .5) +\n  geom_jitter(alpha = .5, width = 0.2)\n\nBased on the boxplot, we‚Äôd expect a t-test to be nonsignificant - the wide boxes of the plots for each condition overlap, even though the mean of condition 2 is slightly higher than condition 1 (the horizontal line through the box)."
  },
  {
    "objectID": "inferences.html#t-test",
    "href": "inferences.html#t-test",
    "title": "7¬† Infer",
    "section": "8.2 t-test",
    "text": "8.2 t-test\nThe first line of code uses the base R command to run a t-test. It tells R to do the test on the data_scalescomputed data frame, asking whether scale1_index varies by (~) condition. var. sets the assumption of variance and conf.level sets the desired alpha of the test. That command is wrapped in the handy t_apa function, which makes the output much easier to read (and pull directly into a write-up!), and also allows us to get a confidence interval on the effect size (es_ci).\nFor reporting, you‚Äôd probably also want to know the means and standard deviations by condition. The next bit of code asks R to do this, using the summarise function, separately for each condition. Reminder to use na.rm here to account for any missing values.\nThe gt() command shows the requested dataframe in a nice formatting.\n\nt_apa(t.test(data = data_scalescomputed, scale1_index ~ condition12, var. = TRUE, conf.level = .95), es_ci = TRUE)\n\n\nscale1_by_condition12 &lt;- data_scalescomputed %&gt;%\n  group_by(condition12) %&gt;%\n  summarise(mean_scale1 = mean(scale1_index, na.rm = TRUE),\n            sd_scale1 = sd(scale1_index, na.rm = TRUE))\n\n\ngt(scale1_by_condition12)"
  },
  {
    "objectID": "describe.html#getting-a-feel-for-your-data",
    "href": "describe.html#getting-a-feel-for-your-data",
    "title": "5¬† Describe",
    "section": "5.1 getting a feel for your data",
    "text": "5.1 getting a feel for your data\nstr\nglimpse\nskimr"
  },
  {
    "objectID": "describe.html#counting-things",
    "href": "describe.html#counting-things",
    "title": "5¬† Describe",
    "section": "5.2 counting things",
    "text": "5.2 counting things\ngroup_by + summarise + count\nn\ntabyl"
  },
  {
    "objectID": "describe.html#getting-descriptives",
    "href": "describe.html#getting-descriptives",
    "title": "5¬† Describe",
    "section": "5.3 getting descriptives",
    "text": "5.3 getting descriptives\ngroup_by + summarise + mean & sd\n\nscale1_by_condition12 &lt;- data_scalescomputed %&gt;%\n  group_by(condition12) %&gt;%\n  summarise(mean_scale1 = mean(scale1_index, na.rm = TRUE),\n            sd_scale1 = sd(scale1_index, na.rm = TRUE))\n\n\n5.3.1 Three things to remember\n\nWhen we compute means, we need to set the decimals via round().\nWe also need to tell R to calculate the mean, even if some of the contributing data points are missing. This is what na.rm = TRUE does.\nAs noted above, rowwise asks R to do something for each row (which is what we want here ‚Äì to compute the mean of the contributing items for each participant). Whenever we use rowwise (or group_by), we need to ungroup() at the end to avoid issues down the line."
  },
  {
    "objectID": "describe.html#tables",
    "href": "describe.html#tables",
    "title": "5¬† Describe",
    "section": "5.4 tables??",
    "text": "5.4 tables??\ngt\n\ngt(scale1_by_condition12)\n\napaTable"
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "3¬† Import",
    "section": "",
    "text": "4 Read in the data\nRemember the file setup described above? This is where that starts to be important. Remember, our working directory (i.e., where R thinks ‚Äúhere‚Äù is) was set via the Rproj file ‚Äì so it is the ‚ÄúWilliams Lab Core R‚Äù folder. You can check this by typing getwd() or here() in the console.\nFor most of this core script, we‚Äôll be using data from a file called sampledata.sav, which should be in the data subfolder from the zipped file. If not, sort that out now!\nA .sav file is in SPSS format. When you export from Qualtrics into .sav/SPSS format, it retains helpful information like question wording and response labels. If you export straight to .csv, you lose that info and will find yourself cross-checking back to Qualtrics. So, strong word of advice to always export to .sav.\nThe code below uses the here command to direct R to the data folder from the working directory, and then the .sav file within it.\nThe glimpse command gives a nice overview of the variables, their type, and a preview of the data.\ndata &lt;- read_sav(here(\"data\", \"sampledata.sav\")) \n\nglimpse(data)\nThese variable names won‚Äôt be very nice to work with with awkward and inconsistent capitalisation. Actual Qualtrics exports are even messier!\nThe clean_names function from janitor helps clean them up!\ndata_cleanednames &lt;- at the start of the line saves the change to a new dataframe. Alternately, you could write it back to the same dataframe (e.g., data &lt;- ), but this should be done very intentionally as it makes it harder to backtrack to the source of an error. The general rule is to create a new dataframe each time you implement a big change on the data.\nThe glimpse command here shows you that you effectively cleaned the variable names!\ndata_cleanednames &lt;- clean_names(data)\n\nglimpse(data_cleanednames)\nA few things about working with files in SPSS format (.sav) before we continue. The reason why we bother with this is that the SPSS format maximises the information in the file. Unlike exporting to .csv or another spreadsheet format, .sav retains information about question wording (saved as a variable label) and response labelling (saved as a value label).\nIf you look at the variable types at the right of the glimpse output, you‚Äôll see the some of the variables are dbl (numeric) while some are dbl+lbl (numeric with labelled values). If you view the data object (by clicking on it in the Environment or using view(data)) you will see that some of the variables have the question wording below the variable name.\nHaving this information on hand is really helpful when working with your data!\nThe view_df function from the sjPlot package creates a really nicely formatted html file that includes variable names, question wording, response options, and response labelling. This code saves the html file to the output_files folder using the here package (which starts where your Rproj file is). This html file is nice as a reference for your own use or to share with a supervisor or collaborator!\nview_df(data_cleanednames)\n\nview_df(data_cleanednames, file=here(\"output_files\",\"spsstest_codebook.html\"))\nThe data_dict function from surveytoolbox makes a dataframe with all the variable and response labels - similar to the html created above, but this can be called upon later in R as it‚Äôs now part of the environment.\ndatadictionary &lt;- data_cleanednames %&gt;%\n  data_dict()\nLet‚Äôs say you just want to know the question wording or response labels for a particular variable, you can do this via code rather than checking the whole dataset. The extract_vallab command from surveytoolbox returns the value labels for a given variable.\ndata_cleanednames %&gt;%\n  extract_vallab(\"demographicscateg\")\nThere are (evidently) times when packages do not like labelled data. So, here are a few tools for removing labels from the haven package. Keep these up your sleeve for problem solving later! zap_labels and zap_label not surprisingly removes the labels - the first removes the value labels and the second removes the variable labels! The code below makes a new data dictionary of the zapped dataframe and glimpses the new dataframe to confirm the labels are gone.\ndata_zapped &lt;- data_cleanednames %&gt;%\n  zap_labels() %&gt;%\n  zap_label()\n\ndatadictionary_zapped &lt;- data_zapped %&gt;%\n  data_dict()\n\nglimpse(data_zapped)\nFor the rest of this script, we will work with the zapped dataframe. This is the recommended approach to save headaches with errors down the line."
  },
  {
    "objectID": "appendix.html#useful-packages-for-psychology",
    "href": "appendix.html#useful-packages-for-psychology",
    "title": "10¬† Appendix",
    "section": "12.3 useful packages for psychology",
    "text": "12.3 useful packages for psychology\n\ntidyverse this is a cluster of super helpful data wrangling and visualisation tools.\nhere this package helps direct R to the correct place for files based on the current working directory.\njanitor this package helps us clean up data - especially awkward variable names.\nqualtRics this package is helpful in reading in data files from Qualtrics‚Ä¶ except for .sav SPSS format files! (see next)\nhaven this package is a good one for reading in .sav SPSS format files\nsjplot this package is helpful for making a ‚Äòcodebook‚Äô of your variables and values from imported .sav files\nsurveytoolbox this package is helpful in drawing out the value labels of variables imported in .sav format ‚Äì note: because surveytoolbox is on github and not CRAN, you‚Äôll want to do the following two steps in the console. Note that we do this in the console since we only need to do it once! If the install asks you about updating packages, go ahead and do it! ‚Äî(1) install the devtools package: install.packages(‚Äúdevtools‚Äù) ‚Äî(2) install via github: devtools::install_github(‚Äúmartinctc/surveytoolbox‚Äù)\nufs this package (short for user friendly science) is a nice tool for computing the internal reliability of scales ‚Äì note: one of the commands we will use in ufs requires the psych package to be installed (but doesn‚Äôt need to be loaded via library()). Ensure you install that first. Two steps: ‚Äî-(1) install the `remotes`` package: install.packages(‚Äúremotes‚Äù) ‚Äî-(2) install via github_lab: remotes::install_gitlab(‚Äòr-packages/ufs‚Äô)\napa nice for making statistical output into APA style\ngt nice for making your tables look pretty\napaTables makes nice APA-styled tables of correlation, ANOVA, regression etc. output\nreport is a package to help with results reporting\npsych is an umbrella package for lots of common psych tasks\nez is a great package for stats, including analysis of variance\nemmeans is helpful for comparing specific means in a factorial design"
  },
  {
    "objectID": "plot.html#plotting-raw-data",
    "href": "plot.html#plotting-raw-data",
    "title": "6¬† Plot",
    "section": "6.1 Plotting raw data",
    "text": "6.1 Plotting raw data\ngeom_point\ngeom_histogram\ngeom_boxplot\ngeom_violin"
  },
  {
    "objectID": "plot.html#plotting-data-summaries",
    "href": "plot.html#plotting-data-summaries",
    "title": "6¬† Plot",
    "section": "6.2 Plotting data summaries",
    "text": "6.2 Plotting data summaries\ngeom_col"
  }
]